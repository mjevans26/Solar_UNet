{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnHMtxTUavbx"
      },
      "source": [
        "#@title Author: Michael Evans { display-mode: \"form\" }\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ciecm6Ia2Xa"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook demonstrates a workflow for training a [fully convolutional neural network (FCNN)](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf), specifically [U-net](https://arxiv.org/abs/1505.04597) on previously exctracted remote sensing data using Tensorflow. In this example, we read 256x256 pixel image chips saved as zipped tfrecords in Google Cloud Storage (Note: the data can be read in from anywhere) containing the visible, infrared, and near infrared bands of Sentinel-2 imagery and a binary label band. This relatively simple model is a mostly unmodified version of [this example](https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/segmentation_blogpost/image_segmentation.ipynb) from the TensorFlow docs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yla55CsQa2yw"
      },
      "source": [
        "from os.path import join\n",
        "from sys import path\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDBvGhTXa5II"
      },
      "source": [
        "## Clone repo containing preprocessing and prediction functions\n",
        "!git clone https://github.com/mjevans26/Satellite_ComputerVision.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dl2DPfr9a8eW"
      },
      "source": [
        "# Load the necessary modules from repo\n",
        "path.append('/content/Satellite_ComputerVision')\n",
        "\n",
        "from utils.processing import get_training_dataset, get_eval_dataset\n",
        "from utils.model_tools import get_model, weighted_bce, make_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoYO47K1gllv"
      },
      "source": [
        "# Specify names locations for outputs in Cloud Storage. \n",
        "BUCKET = '{YOUR_GCS BUCKET HERE}'\n",
        "BUCKET_PATH = join('gs://', BUCKET)\n",
        "\n",
        "FOLDER = 'NC_solar'\n",
        "PRED_BASE = 'data/predict'\n",
        "TRAIN_BASE = 'data/training'\n",
        "EVAL_BASE = 'data/eval'\n",
        "\n",
        "# Specify inputs (Sentinel bands) to the model and the response variable.\n",
        "opticalBands = ['B2', 'B3', 'B4']\n",
        "thermalBands = ['B8', 'B11', 'B12']\n",
        "\n",
        "BANDS = opticalBands + thermalBands# + pcaBands\n",
        "RESPONSE = 'landcover'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBQN2UdagYj6"
      },
      "source": [
        "## Training Data\n",
        "First, we will read previously exported training data fro GCS into TFRecordDatasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7biGY7cbBSU"
      },
      "source": [
        "# make sure we have training records\n",
        "trainPattern = join(BUCKET_PATH, FOLDER, TRAIN_BASE, '*.tfrecord.gz')\n",
        "print(trainPattern)\n",
        "trainFiles = !gsutil ls {trainPattern}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e3C9k5Ugm0R"
      },
      "source": [
        "# create training dataset with default arguments for batch (16), repeat (True), and normalization axis (0)\n",
        "training = get_training_dataset(trainFiles, FEATURES_DICT, BANDS, RESPONSE, 2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhylXyE4g2U2"
      },
      "source": [
        "# confirm the training dataset produces expected results\n",
        "iterator = iter(training)\n",
        "print(iterator.next())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnpFtwj_g_lA"
      },
      "source": [
        "evalPattern = join(BUCKET_PATH, FOLDER, EVAL_BASE, '*.tfrecord.gz')\n",
        "print(evalPattern)\n",
        "evalFiles = !gsutil ls {evalPattern}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js6Dn2dshHYL"
      },
      "source": [
        "# create evaluation dataset\n",
        "evaluation = get_eval_dataset(evalFiles, FEATURES_DICT, BANDS, RESPONSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR06Y089jeSk"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ww2Yq36kbJm"
      },
      "source": [
        "# Define Global variables for Model Training\n",
        "EPOCHS = 100\n",
        "LR = 0.0001\n",
        "BATCH = 16\n",
        "\n",
        "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LR, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "METRICS = {\n",
        "        'logits':[tf.keras.metrics.MeanSquaredError(name='mse'), tf.keras.metrics.Precision(name='precision'), tf.keras.metrics.Recall(name='recall')],\n",
        "        'classes':[tf.keras.metrics.MeanIoU(num_classes=2, name = 'mean_iou')]\n",
        "        }\n",
        "\n",
        "OUT_DIR  = '{YOUR DIRECTORY FOR SAVING MODEL FILES HERE}'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf1VcE0h9ZBj"
      },
      "source": [
        "When our training data is unbalanced it can be helpful to provide weights for the positive examples so that the model doesn't 'learn' to just predict zeros everywhere. To calculate the weight we read through the dataset and count up the number of 1s and 0s in our labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk2tu-Q6l613"
      },
      "source": [
        "# Instantiate a nonsense model\n",
        "m = get_model(depth = len(BANDS), optim = OPTIMIZER, loss = 'mse', mets = [tf.keras.metrics.categorical_accuracy], bias = None)\n",
        "train_con_mat = make_confusion_matrix(training, m)\n",
        "classums = train_con_mat.sum(axis = 1)\n",
        "\n",
        "# Calculate and save Bias, Weight, and Train size based on data\n",
        "BIAS = np.log(classums[1]/classums[0])\n",
        "WEIGHT = classums[0]/classums[1]\n",
        "TRAIN_SIZE = train_con_mat.sum()//(256*256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ouh97-9qP7"
      },
      "source": [
        "During model training we will save the best performing set of weights as calculated on evaluation data at the end of each epoch. THe metric we track is the mean intersection over union."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlRa0mR6kRwY"
      },
      "source": [
        "## DEFINE CALLBACKS\n",
        "\n",
        "def get_weighted_bce(y_true, y_pred):\n",
        "    return weighted_bce(y_true, y_pred, WEIGHT)\n",
        "\n",
        "# get the current time\n",
        "now = datetime.now() \n",
        "date = now.strftime(\"%d%b%y\")\n",
        "date\n",
        "\n",
        "# define a checkpoint callback to save best models during training\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(OUT_DIR, 'best_weights_' + date + '.hdf5'),\n",
        "    monitor='val_classes_mean_iou',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0FluNo_9xzL"
      },
      "source": [
        "Create and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmC4a8c7jfb3"
      },
      "source": [
        "m = get_model(depth = len(BANDS), optim = OPTIMIZER, loss = get_weighted_bce, mets = METRICS, bias = BIAS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mKm-2j3ki6u"
      },
      "source": [
        "# train the model\n",
        "m.fit(\n",
        "        x = training,\n",
        "        epochs = EPOCHS,\n",
        "        steps_per_epoch = int(TRAIN_SIZE//BATCH),\n",
        "        validation_data = evaluation,\n",
        "        callbacks = [checkpoint]\n",
        "        )\n",
        "\n",
        "m.save(os.path.join(OUT_DIR, f'{date}_unet256.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r73nInHK5HkZ"
      },
      "source": [
        "## Re-Training\n",
        " The code below will continue training an existing model. You may need to re-create your training and evaluation datasets if you intend to use new or different data from that on which the model was originally trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKhUr2BI5MjN"
      },
      "source": [
        "from tensorflow.python.keras import models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfpl6-436ajg"
      },
      "source": [
        "# Define where pre-trained model files and weights will come from\n",
        "MODEL_FILE = '{PATH TO .h5 MODEL FILE}'\n",
        "WEIGHT_FILE = '{PATH TO .hdf5 WEIGHT FILE'\n",
        "EVAL_METRIC = 'val_classes_mean_iou'\n",
        "# optionally change the learning rate\n",
        "LR = 0.0001\n",
        "# optionally change the number of epochs to re-train\n",
        "EPOCHS = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujfWjVTc7DG7"
      },
      "source": [
        "# this non-keras native function was used during training so we need to supply it when re-instantiating the trained model\n",
        "def get_weighted_bce(y_true, y_pred):\n",
        "    return weighted_bce(y_true, y_pred, weight)\n",
        "\n",
        "# get the current time\n",
        "now = datetime.now() \n",
        "date = now.strftime(\"%d%b%y\")\n",
        "date\n",
        "\n",
        "# define a checkpoint callback to save best models during training\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    os.path.join(OUT_DIR, 'best_weights_' + date + '.hdf5'),\n",
        "    monitor='val_classes_mean_iou',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='max'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdLb1x9R6XO-"
      },
      "source": [
        "# load our trained model from the model and weights file\n",
        "custom_objects = {'get_weighted_bce': get_weighted_bce}\n",
        "m = models.load_model(MODEL_FILE, custom_objects = custom_objects)\n",
        "m.load_weights(WEIGHT_FILE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk7X7tC66nlD"
      },
      "source": [
        "# set the initial evaluation metric for saving checkpoints to the previous best value\n",
        "evalMetrics = m.evaluate(x = eval_data, verbose = 1)\n",
        "metrics = m.metrics_names\n",
        "index = metrics.index(EVAL_METRIC)\n",
        "checkpoint.best = evalMetrics[index]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1xL8CEZ7VNs"
      },
      "source": [
        "# OPTIONALLY set the learning rate for re-training\n",
        "lr = backend.eval(m.optimizer.learning_rate)\n",
        "print('current learning rate', lr)\n",
        "backend.set_value(m.optimizer.learning_rate, LR)\n",
        "print('new learning rate', LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbSVDeJz7fem"
      },
      "source": [
        "# train the model\n",
        "m.fit(\n",
        "        x = training,\n",
        "        epochs = EPOCHS,\n",
        "        steps_per_epoch = steps_per_epoch,\n",
        "        validation_data = evaluation,\n",
        "        callbacks = [checkpoint]\n",
        "        )\n",
        "\n",
        "m.save(os.path.join(OUT_DIR, f'{date}_unet256.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}